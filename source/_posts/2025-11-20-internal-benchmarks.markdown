---
title: Internal Benchmarks
kind: article
author: Atharva Raykar
created_at: 2025-11-20 00:00:00 UTC
layout: post
---
A few months ago, I was co-facilitating [a "Birds of a Feather" session](https://hasgeek.com/fifthelephant/2025/sub/birds-of-feather-bof-session-finding-signal-in-a-n-8hrRznGe3qf6e7zXxKwcDi) on keeping up with AI progress.

A big talking point that came up from the ICs and engineering leaders that participated was that popular public benchmarks are insufficient for determining if an AI model is a good fit for their product.

![](/images/blog/bof.jpg)

*Pictured above, clockwise: (1) My co-facilitator Lavanya Tekumala. (2) Developers talking about benchmarks and AI-assisted coding. (3) The whiteboard from the session which featured the word "benchmark" three times.*

I want to sharpen this observation a bit more.

## "The vibes"

- my colleagues' preferences

## Benchmark illusions

## Models aren't fungible

## "Evals", Internal Benchmarks and Public Benchmarks


## Aligned incentives


## Minimum viable benchmark

