---
title: "Exploring text-to-SQL "
kind: article
author: Sezal Jain
created_at: 2025-04-30 00:00:00 UTC
layout: post
---
With LLMs becoming useful in coding and producing structured data, SQL generation from natural language questions is a problem with thousands of solutions built over the past year. But, no standard approach has been established, leaving room for further exploration and improvement.

Writing SQL queries to get the required data is a core need for all organizations. These Text-to-SQL tools don’t eliminate the need for SQL knowledge/expertise completely; rather they reduce the turnaround time and enable rapid experimentation and exploration. Analysts still review and validate every query, but they spend far less time hand-coding and iterating. This isn't merely about technical convenience, it's about empowering entire teams to participate in data-driven decision-making, thereby driving business agility.

There are other business problems where LLMs can be used effectively, but we are focusing first on text-to-SQL because:

- Availability of benchmark datasets for evaluation (Bird Bench, Spider SQL etc)
- Accurate evaluation by executing the generated and reference queries
- An immediately usable product for organizations



> How to take text-to-SQL beyond a proof of concept, and create a robust product which is valuable to organizations


One of the surveys[^1] done on text-to-SQL explores more than a 100 papers on SQL generation and the different techniques used to improve the quality/accuracy of the sql generated. At nilenso, we are exploring such techniques, looking at their advantages, ease of application and relevance for real world usecases.

### In Context Learning

For this initial exploration, we focused on **in-context learning** (ICL) with off-the-shelf models to translate natural-language queries to SQL, deliberately excluding fine-tuning. We also reduce our reliance on detailed table and column descriptions, recognizing that such metadata is often scarce in real-world scenarios.

We are currently exploring the following approaches for ICL.

- RAG Architecture
- Declarative agentic workflow
- Reactive agentic workflow

There are common techniques we end up using across these approaches, like formatting a schema a certain way, breaking down queries etc. 

### Benchmarking

[Bird-Bench](https://bird-bench.github.io/) is an extensive cross-domain database that contains over **12,751** unique question-SQL pairs spread over different domains and datasets, with ~1500 questions in its dev-set.  For our experimentation, we have first focused on the 1500 questions in the dev-set.

A sample entry in the questions can look like:
A sample entry in the questions can look like:

```
  {
    "question_id": 163,
    "db_id": "financial",
    "question": "Which district has the most accounts with loan contracts finished with no problems?",
    "evidence": "status = 'A' refers to loan contracts finished with no problems",
    "SQL": "SELECT T1.A2 FROM District AS T1 INNER JOIN Account AS T2 ON T1.District_id = T2.District_id INNER JOIN Loan AS T3 ON T2.Account_id = T3.Account_id WHERE T3.status = 'A' GROUP BY T1.District_id ORDER BY COUNT(T2.Account_id) DESC LIMIT 1",
    "difficulty": "moderate"
  },
```

This gives you the input NL question as well as the reference-SQL generated by data engineers. We can now evaluate the generated SQL against the reference with different methods easily.

Bird Bench dataset will be used in all our approaches as a benchmark to understand what techniques improve the solution and which dont. We will also be benchmarking on other datasets like [Spider-SQL](https://yale-lily.github.io/spider), which follows a very similar structure.

### Evaluation:

The methods to evaluate the generated vs reference SQL are broadly divided into content matching (exact string or component matching), query execution or LLM-as-judge based evaluation methods.

We focus on execution based methods for our experiments. We match the data generated by the two queries, but don’t fail evaluation in the following cases:

- The column names or order don’t matter
- If there are extra columns generated by the query, its ok. Sometimes additional information is useful to the analyst
- Row order is important in some cases, not all

These evaluation metrics help us to focus on techniques which will be useful in the real world ***helping an analyst*** usecases.

We will be writing about our experiments with the different approaches, and our opinions and observations while implementing them in upcoming blogs.

References:  
[^1]: [Next-Generation Database Interfaces: A Survey of LLM-based Text-to-SQL](https://arxiv.org/pdf/2406.08426v3) This is an anlaysis of recent advances in LLM-based text-to-SQL